{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJpKaj1YGQZF"
      },
      "source": [
        "# Dialect Conversion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oUXJxeBGY70"
      },
      "source": [
        "convert text between UK and US dialects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn7KN3aDGhUe"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3rPmdS0RC4S",
        "outputId": "7557a74f-3544-4709-af1c-f212f5e57ccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in d:\\program_file\\anaconda\\lib\\site-packages (4.50.0)\n",
            "Requirement already satisfied: datasets in d:\\program_file\\anaconda\\lib\\site-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in d:\\program_file\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in d:\\program_file\\anaconda\\lib\\site-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\program_file\\anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\program_file\\anaconda\\lib\\site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\program_file\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\program_file\\anaconda\\lib\\site-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in d:\\program_file\\anaconda\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\program_file\\anaconda\\lib\\site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in d:\\program_file\\anaconda\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in d:\\program_file\\anaconda\\lib\\site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in d:\\program_file\\anaconda\\lib\\site-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\program_file\\anaconda\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in d:\\program_file\\anaconda\\lib\\site-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in d:\\program_file\\anaconda\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in d:\\program_file\\anaconda\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in d:\\program_file\\anaconda\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in d:\\program_file\\anaconda\\lib\\site-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\program_file\\anaconda\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in d:\\program_file\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\program_file\\anaconda\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\program_file\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\program_file\\anaconda\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\program_file\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\program_file\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\program_file\\anaconda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\program_file\\anaconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\program_file\\anaconda\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\program_file\\anaconda\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: colorama in d:\\program_file\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\program_file\\anaconda\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\program_file\\anaconda\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\program_file\\anaconda\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in d:\\program_file\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ex_Z4yNGmfE"
      },
      "source": [
        "* Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b-vCUhs5G6_C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "UHbpnHnlHTNd",
        "outputId": "ace93587-7827-4431-812f-68dfd7a1d64f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "input_text",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "target_text",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "8e5bf64a-6c62-40d4-bf4c-776b98bbef61",
              "rows": [
                [
                  "0",
                  "I CoLoUr üé® the centre of my favourite book.",
                  "I color the center of my favorite book."
                ],
                [
                  "1",
                  "He is travelling ‚úàÔ∏è to the THEATRE.",
                  "He is traveling to the theater."
                ],
                [
                  "2",
                  "I have a flat near the lift.",
                  "I have an apartment near the elevator."
                ],
                [
                  "3",
                  "I have a flat near the lift. ",
                  "I have an apartment near the elevator."
                ],
                [
                  "4",
                  "The PROGRAMME üóìÔ∏è will start at 6 O'CLOCK.",
                  "The program will start at 6 o'clock."
                ],
                [
                  "5",
                  "HE has a cheque üí≥ for payment.",
                  "He has a check for payment."
                ],
                [
                  "6",
                  "She wears jewellery üíé on occasions...",
                  "She wears jewelry on occasions."
                ],
                [
                  "7",
                  " THEY are Practising   ‚öΩ for the football MATCH.",
                  "They are practicing for the soccer game."
                ],
                [
                  "8",
                  "He is using a spanner for the repair.",
                  "He is using a wrench for the repair."
                ],
                [
                  "9",
                  "The aeroplane ‚úàÔ∏è landed on time.",
                  "The airplane landed on time."
                ],
                [
                  "10",
                  "hello... üòÉ how are you?",
                  "hello... üòÉ how are you?"
                ],
                [
                  "11",
                  "She bought some colour pencils.",
                  "She bought some color pencils."
                ],
                [
                  "12",
                  "I am going to the lift.",
                  "I am going to the elevator."
                ],
                [
                  "13",
                  "His behaviour ü§î is unacceptable.",
                  "His behavior is unacceptable."
                ],
                [
                  "14",
                  "The cheque üí≥ arrived late üò¢.",
                  "The check arrived late."
                ],
                [
                  "15",
                  "Do you know where the lift is?",
                  "Do you know where the elevator is?"
                ],
                [
                  "16",
                  "The labor union is organizing a programme üóìÔ∏è.",
                  "The labour union is organizing a program."
                ],
                [
                  "17",
                  "He enjoys playing football ‚öΩ.",
                  "He enjoys playing soccer."
                ],
                [
                  "18",
                  "I love visiting the theatre.",
                  "I love visiting the theater."
                ],
                [
                  "19",
                  "Their practise sessions are improving.",
                  "Their practice sessions are improving."
                ],
                [
                  "20",
                  "He likes the colour red.",
                  "He likes the color red."
                ],
                [
                  "21",
                  "The cheque has been approved.",
                  "The check has been approved."
                ],
                [
                  "22",
                  "The aeroplane ‚úàÔ∏è was delayed.",
                  "The airplane was delayed."
                ],
                [
                  "23",
                  "Their neighbourhood is beautiful.",
                  "Their neighborhood is beautiful."
                ],
                [
                  "24",
                  "They've cancelled the programme.",
                  "They've canceled the program."
                ],
                [
                  "25",
                  "She practises yoga regularly.",
                  "She practices yoga regularly."
                ],
                [
                  "26",
                  "The cheque has not arrived yet.",
                  "The check has not arrived yet."
                ],
                [
                  "27",
                  "He is organizing a theatre play.",
                  "He is organizing a theater play."
                ],
                [
                  "28",
                  "I prefer the lift to the stairs.",
                  "I prefer the elevator to the stairs."
                ],
                [
                  "29",
                  "His behaviour has been exemplary.",
                  "His behavior has been exemplary."
                ],
                [
                  "30",
                  "Is the cheque ready for collection?",
                  "Is the check ready for collection?"
                ],
                [
                  "31",
                  "Please colour üé® this drawing.",
                  "Please color this drawing."
                ],
                [
                  "32",
                  "The aeroplane ‚úàÔ∏è has landed safely.",
                  "The airplane has landed safely."
                ],
                [
                  "33",
                  "They're still practising football ‚öΩ.",
                  "They're still practicing soccer."
                ],
                [
                  "34",
                  "Her jewellery collection is stunning.",
                  "Her jewelry collection is stunning."
                ],
                [
                  "35",
                  "What's the programme for tomorrow?",
                  "What's the program for tomorrow?"
                ],
                [
                  "36",
                  "Their labour union is powerful.",
                  "Their labor union is powerful."
                ],
                [
                  "37",
                  "They enjoy going to the theatre.",
                  "They enjoy going to the theater."
                ],
                [
                  "38",
                  "Her favourite dish is lasagna.",
                  "Her favorite dish is lasagna."
                ],
                [
                  "39",
                  "I need to go to the flat.",
                  "I need to go to the apartment."
                ],
                [
                  "40",
                  "The cheque is invalid.",
                  "The check is invalid."
                ],
                [
                  "41",
                  "The aeroplane ‚úàÔ∏è is ready for boarding.",
                  "The airplane is ready for boarding."
                ],
                [
                  "42",
                  "He prefers the colour blue.",
                  "He prefers the color blue."
                ],
                [
                  "43",
                  "The theatre play was amazing.",
                  "The theater play was amazing."
                ],
                [
                  "44",
                  "The programme üóìÔ∏è starts at 10 AM.",
                  "The program starts at 10 AM."
                ],
                [
                  "45",
                  "Their neighbourhood is very welcoming.",
                  "Their neighborhood is very welcoming."
                ],
                [
                  "46",
                  "Please practise before the event.",
                  "Please practice before the event."
                ],
                [
                  "47",
                  "Her jewellery is antique.",
                  "Her jewelry is antique."
                ],
                [
                  "48",
                  "The cheque üí≥ bounced.",
                  "The check bounced."
                ],
                [
                  "49",
                  "She wears jewellery every day.",
                  "She wears jewelry every day."
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 96
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_text</th>\n",
              "      <th>target_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I CoLoUr üé® the centre of my favourite book.</td>\n",
              "      <td>I color the center of my favorite book.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>He is travelling ‚úàÔ∏è to the THEATRE.</td>\n",
              "      <td>He is traveling to the theater.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I have a flat near the lift.</td>\n",
              "      <td>I have an apartment near the elevator.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I have a flat near the lift.</td>\n",
              "      <td>I have an apartment near the elevator.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The PROGRAMME üóìÔ∏è will start at 6 O'CLOCK.</td>\n",
              "      <td>The program will start at 6 o'clock.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>The theatre's performance was breathtaking.</td>\n",
              "      <td>The theater's performance was breathtaking.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>Her behaviour has been commendable.</td>\n",
              "      <td>Her behavior has been commendable.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>The cheque was never received.</td>\n",
              "      <td>The check was never received.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>The aeroplane ‚úàÔ∏è took off on time.</td>\n",
              "      <td>The airplane took off on time.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>She wears jewellery for special occasions.</td>\n",
              "      <td>She wears jewelry for special occasions.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>96 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     input_text  \\\n",
              "0   I CoLoUr üé® the centre of my favourite book.   \n",
              "1           He is travelling ‚úàÔ∏è to the THEATRE.   \n",
              "2                  I have a flat near the lift.   \n",
              "3                 I have a flat near the lift.    \n",
              "4     The PROGRAMME üóìÔ∏è will start at 6 O'CLOCK.   \n",
              "..                                          ...   \n",
              "91  The theatre's performance was breathtaking.   \n",
              "92          Her behaviour has been commendable.   \n",
              "93               The cheque was never received.   \n",
              "94           The aeroplane ‚úàÔ∏è took off on time.   \n",
              "95   She wears jewellery for special occasions.   \n",
              "\n",
              "                                    target_text  \n",
              "0       I color the center of my favorite book.  \n",
              "1               He is traveling to the theater.  \n",
              "2        I have an apartment near the elevator.  \n",
              "3        I have an apartment near the elevator.  \n",
              "4          The program will start at 6 o'clock.  \n",
              "..                                          ...  \n",
              "91  The theater's performance was breathtaking.  \n",
              "92           Her behavior has been commendable.  \n",
              "93                The check was never received.  \n",
              "94               The airplane took off on time.  \n",
              "95     She wears jewelry for special occasions.  \n",
              "\n",
              "[96 rows x 2 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file=r\"D:\\Data centr\\Exel_data\\int data\\CozmoX Assignment Dataset.csv\"\n",
        "data=pd.read_csv(file)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iShIWMC_IjJU",
        "outputId": "2536e4b9-2227-4d9a-bf83-b4993a30245d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 96 entries, 0 to 95\n",
            "Data columns (total 2 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   input_text   96 non-null     object\n",
            " 1   target_text  96 non-null     object\n",
            "dtypes: object(2)\n",
            "memory usage: 1.6+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(data.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "tlZ1Sp5sIwve",
        "outputId": "a6cd5a77-86db-4200-8e0d-3c23404cc345"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "input_text     0\n",
              "target_text    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7dN5WXSGu2f"
      },
      "source": [
        "* Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J8aDHh0EJV-k"
      },
      "outputs": [],
      "source": [
        "import unicodedata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IYMW8Gm7G7qD"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  text=unicodedata.normalize(\"NFKC\", text) # Normalize the char\n",
        "  text=text.lower()\n",
        "  text=re.sub(r\"[^\\w\\s]\", \"\", text)  # remove special characters & Emojis\n",
        "  text=re.sub(r\"\\s+\", \" \", text)  # remove extra spaces\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX7FctnWLAcR"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fzq2d-IFKVPd"
      },
      "outputs": [],
      "source": [
        "data[\"input_text\"] = data[\"input_text\"].apply(clean_text)\n",
        "data[\"target_text\"] = data[\"target_text\"].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb9bvYLjLEFD",
        "outputId": "2b0d5ef4-7395-4c50-9b6a-1a44ed24520c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                 input_text  \\\n",
            "0  i colour the centre of my favourite book   \n",
            "1           he is travelling to the theatre   \n",
            "2               i have a flat near the lift   \n",
            "3              i have a flat near the lift    \n",
            "4      the programme will start at 6 oclock   \n",
            "\n",
            "                              target_text  \n",
            "0  i color the center of my favorite book  \n",
            "1          he is traveling to the theater  \n",
            "2   i have an apartment near the elevator  \n",
            "3   i have an apartment near the elevator  \n",
            "4      the program will start at 6 oclock  \n"
          ]
        }
      ],
      "source": [
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibVmbRfULV6C",
        "outputId": "fa71cab4-e489-4482-d945-c79953468a64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " saved\n"
          ]
        }
      ],
      "source": [
        "data.to_csv(\"processed_cozmoX.csv\", index=False)\n",
        "print(\"\\n saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LZ7GxY0PEes"
      },
      "source": [
        "# Model Selection- T5\n",
        "Pretrained on Text Transformation.\n",
        "\n",
        "Best for small dataset with accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YGyMCYZoPRlp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, TFAutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkZyZE_6Gza0"
      },
      "source": [
        "* Tokenization- breaking text into smaller units (tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWtWgCm68mpZ",
        "outputId": "a55cb2a5-ec3b-4e3d-cbf0-ecfe61763aaa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "454128625c074bcd86ad71c69d2a6fb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Program_file\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\himan\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aaf2b56e44a34e8ea07d52ed979bb2ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "284d897f97394517a1429813c3024419",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "tokanizer=T5Tokenizer.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p0p6YeXJ3Q_",
        "outputId": "2318753c-39a8-44dd-b923-3cc41df10e29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\Program_file\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afbf08eccd8c45849096f33f399b75b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\Program_file\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "model_t5=TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BU6XScMR_0X_"
      },
      "outputs": [],
      "source": [
        "def tokenize_data(sentence):\n",
        "  input_text = \"translate UK to US: \" + sentence[\"input_text\"]  # Task prefix\n",
        "  target_text = sentence[\"target_text\"]\n",
        "\n",
        "  tokenized_inputs = tokanizer(input_text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"tf\")\n",
        "  tokenized_targets=tokanizer(target_text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"tf\")\n",
        "\n",
        "  return{\n",
        "      \"input_ids\": tokenized_inputs[\"input_ids\"][0],  # Extract tensors\n",
        "      \"attention_mask\": tokenized_inputs[\"attention_mask\"][0],\n",
        "      \"labels\": tokenized_targets[\"input_ids\"][0],\n",
        "      }\n",
        "\n",
        "token_dataset=data.apply(tokenize_data, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2ca7q-gVr8U"
      },
      "source": [
        "Convert the tokenized dataset to a TensorFlow dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "z1irxqIw_0NT"
      },
      "outputs": [],
      "source": [
        "def to_tf(tokenized_data):\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "  labels=[]\n",
        "\n",
        "  for item in tokenized_data:\n",
        "    input_ids.append(item[\"input_ids\"])\n",
        "    attention_masks.append(item[\"attention_mask\"])\n",
        "    labels.append(item[\"labels\"])\n",
        "\n",
        "  return tf.data.Dataset.from_tensor_slices(({\n",
        "      \"input_ids\": input_ids,\n",
        "      \"attention_mask\": attention_masks\n",
        "  },\n",
        "      tf.stack(labels)\n",
        "  ))\n",
        "  tf_dataset=to_tf(token_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-tfRDAqWYUj"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lQF-b-XIKnMD"
      },
      "outputs": [],
      "source": [
        "lr_sechedule=tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.9,\n",
        "    staircase=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yycGVc55Wxbp"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_sechedule)\n",
        "loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPEuvWo7Y8St",
        "outputId": "c515238c-2ca7-4bfb-d5d8-54b9b0cf8db8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: [12.706328]\n",
            "Loss: [2.244296]\n",
            "Loss: [0.2505015]\n",
            "Loss: [0.35311702]\n",
            "Loss: [0.39860797]\n",
            "Loss: [0.24281064]\n",
            "Loss: [0.22058557]\n",
            "Loss: [0.25443977]\n",
            "Loss: [0.19203979]\n",
            "Loss: [0.11452226]\n",
            "Loss: [0.13167979]\n",
            "Loss: [0.08652425]\n",
            "Loss: [0.10950971]\n",
            "Loss: [0.11259355]\n",
            "Loss: [0.93104804]\n",
            "Loss: [0.17465252]\n",
            "Loss: [0.21873431]\n",
            "Loss: [0.24966344]\n",
            "Loss: [0.23980623]\n",
            "Loss: [0.20394443]\n",
            "Loss: [0.22640795]\n",
            "Loss: [0.23582111]\n",
            "Loss: [0.24417078]\n",
            "Loss: [0.20024209]\n",
            "Loss: [0.24859692]\n",
            "Loss: [0.16590907]\n",
            "Loss: [0.20332238]\n",
            "Loss: [0.18894307]\n",
            "Loss: [0.14160718]\n",
            "Loss: [0.10031591]\n",
            "Loss: [0.22112861]\n",
            "Loss: [0.1478179]\n",
            "Loss: [0.10265511]\n",
            "Loss: [0.11954484]\n",
            "Loss: [0.08252809]\n",
            "Loss: [0.08896951]\n",
            "Loss: [0.09104055]\n",
            "Loss: [0.12273654]\n",
            "Loss: [0.05601408]\n",
            "Loss: [0.09164749]\n",
            "Loss: [0.20163625]\n",
            "Loss: [0.07711082]\n",
            "Loss: [0.06035097]\n",
            "Loss: [0.10470805]\n",
            "Loss: [0.10274045]\n",
            "Loss: [0.05791936]\n",
            "Loss: [0.07011247]\n",
            "Loss: [0.0524703]\n",
            "Loss: [0.15679812]\n",
            "Loss: [0.04060385]\n",
            "Loss: [0.06277525]\n",
            "Loss: [0.04426995]\n",
            "Loss: [0.11738274]\n",
            "Loss: [0.06120779]\n",
            "Loss: [0.07011108]\n",
            "Loss: [0.1561971]\n",
            "Loss: [0.09448341]\n",
            "Loss: [0.07706451]\n",
            "Loss: [0.06273616]\n",
            "Loss: [0.11428536]\n",
            "Loss: [0.06932207]\n",
            "Loss: [0.08050513]\n",
            "Loss: [0.09276798]\n",
            "Loss: [0.11273383]\n",
            "Loss: [0.0429851]\n",
            "Loss: [0.05996889]\n",
            "Loss: [0.08253974]\n",
            "Loss: [0.0811215]\n",
            "Loss: [0.03540509]\n",
            "Loss: [0.05664033]\n",
            "Loss: [0.03118668]\n",
            "Loss: [0.02319995]\n",
            "Loss: [0.02326233]\n",
            "Loss: [0.06580572]\n",
            "Loss: [0.04490199]\n",
            "Loss: [0.04027741]\n",
            "Loss: [0.06229507]\n",
            "Loss: [0.08314867]\n",
            "Loss: [0.03509881]\n",
            "Loss: [0.04701915]\n",
            "Loss: [0.06284907]\n",
            "Loss: [0.04642884]\n",
            "Loss: [0.06168455]\n",
            "Loss: [0.05525783]\n",
            "Loss: [0.05111326]\n",
            "Loss: [0.07918271]\n",
            "Loss: [0.0873204]\n",
            "Loss: [0.07552361]\n",
            "Loss: [0.03806213]\n",
            "Loss: [0.05601149]\n",
            "Loss: [0.06757446]\n",
            "Loss: [0.04751891]\n",
            "Loss: [0.07709134]\n",
            "Loss: [0.05082668]\n",
            "Loss: [0.08046633]\n",
            "Loss: [0.00354293]\n",
            "Completed epoch 1\n",
            "Loss: [0.07008693]\n",
            "Loss: [0.01766379]\n",
            "Loss: [0.0446764]\n",
            "Loss: [0.03277596]\n",
            "Loss: [0.11093778]\n",
            "Loss: [0.00803584]\n",
            "Loss: [0.01023878]\n",
            "Loss: [0.0540451]\n",
            "Loss: [0.03299583]\n",
            "Loss: [0.00301323]\n",
            "Loss: [0.01880077]\n",
            "Loss: [0.00294117]\n",
            "Loss: [0.00805455]\n",
            "Loss: [0.00446908]\n",
            "Loss: [0.01799589]\n",
            "Loss: [0.0060772]\n",
            "Loss: [0.0668347]\n",
            "Loss: [0.00356132]\n",
            "Loss: [0.00741331]\n",
            "Loss: [0.01004602]\n",
            "Loss: [0.00349397]\n",
            "Loss: [0.00260725]\n",
            "Loss: [0.00136852]\n",
            "Loss: [0.00197013]\n",
            "Loss: [0.03358735]\n",
            "Loss: [0.00440476]\n",
            "Loss: [0.00116103]\n",
            "Loss: [0.00185007]\n",
            "Loss: [0.00556714]\n",
            "Loss: [0.00201765]\n",
            "Loss: [0.00999988]\n",
            "Loss: [0.00371336]\n",
            "Loss: [0.00089188]\n",
            "Loss: [0.004188]\n",
            "Loss: [0.00457659]\n",
            "Loss: [0.00058808]\n",
            "Loss: [0.00657279]\n",
            "Loss: [0.00213995]\n",
            "Loss: [0.00344623]\n",
            "Loss: [0.00320174]\n",
            "Loss: [0.00210084]\n",
            "Loss: [0.0004647]\n",
            "Loss: [0.00080858]\n",
            "Loss: [0.004197]\n",
            "Loss: [0.03775536]\n",
            "Loss: [0.00082646]\n",
            "Loss: [0.00131716]\n",
            "Loss: [0.0015306]\n",
            "Loss: [0.00084569]\n",
            "Loss: [0.00065775]\n",
            "Loss: [0.0007032]\n",
            "Loss: [0.00135183]\n",
            "Loss: [0.00064417]\n",
            "Loss: [0.00117778]\n",
            "Loss: [0.0004022]\n",
            "Loss: [0.02351786]\n",
            "Loss: [0.00088188]\n",
            "Loss: [0.00479501]\n",
            "Loss: [0.00147169]\n",
            "Loss: [0.00055415]\n",
            "Loss: [0.0006769]\n",
            "Loss: [0.00028288]\n",
            "Loss: [0.0012093]\n",
            "Loss: [0.0025705]\n",
            "Loss: [0.00027121]\n",
            "Loss: [0.00045654]\n",
            "Loss: [0.00031989]\n",
            "Loss: [0.0012962]\n",
            "Loss: [0.00027044]\n",
            "Loss: [0.00156728]\n",
            "Loss: [0.00072165]\n",
            "Loss: [0.00068719]\n",
            "Loss: [0.00043813]\n",
            "Loss: [0.00110037]\n",
            "Loss: [0.00055047]\n",
            "Loss: [0.0005174]\n",
            "Loss: [0.00034968]\n",
            "Loss: [0.00019129]\n",
            "Loss: [0.00038001]\n",
            "Loss: [0.00028766]\n",
            "Loss: [0.00016948]\n",
            "Loss: [0.00041983]\n",
            "Loss: [0.00015062]\n",
            "Loss: [0.00106478]\n",
            "Loss: [0.00034826]\n",
            "Loss: [0.00029863]\n",
            "Loss: [0.00014031]\n",
            "Loss: [0.00011796]\n",
            "Loss: [0.00201859]\n",
            "Loss: [0.00015404]\n",
            "Loss: [0.00023092]\n",
            "Loss: [0.00035199]\n",
            "Loss: [0.00038109]\n",
            "Loss: [0.00018933]\n",
            "Loss: [0.00018198]\n",
            "Loss: [0.00030232]\n",
            "Completed epoch 2\n",
            "Loss: [0.00158328]\n",
            "Loss: [0.00092049]\n",
            "Loss: [0.00248538]\n",
            "Loss: [0.00200832]\n",
            "Loss: [0.00638842]\n",
            "Loss: [0.00149741]\n",
            "Loss: [0.00032477]\n",
            "Loss: [0.09777939]\n",
            "Loss: [0.0004257]\n",
            "Loss: [0.00010291]\n",
            "Loss: [0.00016334]\n",
            "Loss: [0.00161018]\n",
            "Loss: [0.00051906]\n",
            "Loss: [0.00035263]\n",
            "Loss: [0.00013885]\n",
            "Loss: [0.0002494]\n",
            "Loss: [0.02889995]\n",
            "Loss: [0.00058098]\n",
            "Loss: [0.00085673]\n",
            "Loss: [0.00037172]\n",
            "Loss: [0.00043208]\n",
            "Loss: [0.0002345]\n",
            "Loss: [0.00039993]\n",
            "Loss: [0.00032135]\n",
            "Loss: [0.0731574]\n",
            "Loss: [0.00024149]\n",
            "Loss: [0.00025473]\n",
            "Loss: [0.00082479]\n",
            "Loss: [0.00087595]\n",
            "Loss: [0.00040641]\n",
            "Loss: [0.0020109]\n",
            "Loss: [0.00234137]\n",
            "Loss: [0.00091616]\n",
            "Loss: [0.0724882]\n",
            "Loss: [0.00067299]\n",
            "Loss: [0.00412348]\n",
            "Loss: [0.00249487]\n",
            "Loss: [0.00100863]\n",
            "Loss: [0.00053112]\n",
            "Loss: [0.00216679]\n",
            "Loss: [0.00047101]\n",
            "Loss: [0.0005269]\n",
            "Loss: [0.00055359]\n",
            "Loss: [0.00666206]\n",
            "Loss: [0.0020589]\n",
            "Loss: [0.00137408]\n",
            "Loss: [0.00157386]\n",
            "Loss: [0.00039465]\n",
            "Loss: [0.00077889]\n",
            "Loss: [0.00045299]\n",
            "Loss: [0.00089318]\n",
            "Loss: [0.00034012]\n",
            "Loss: [0.00084657]\n",
            "Loss: [0.00251282]\n",
            "Loss: [0.00032204]\n",
            "Loss: [0.00537031]\n",
            "Loss: [0.00033635]\n",
            "Loss: [0.00056776]\n",
            "Loss: [0.00095697]\n",
            "Loss: [0.00031659]\n",
            "Loss: [0.00077612]\n",
            "Loss: [0.00015375]\n",
            "Loss: [0.00029154]\n",
            "Loss: [0.00143078]\n",
            "Loss: [0.0001825]\n",
            "Loss: [0.00067409]\n",
            "Loss: [0.00016966]\n",
            "Loss: [0.00097176]\n",
            "Loss: [0.00094141]\n",
            "Loss: [0.00050641]\n",
            "Loss: [0.00050915]\n",
            "Loss: [0.00040447]\n",
            "Loss: [0.00014375]\n",
            "Loss: [0.00039244]\n",
            "Loss: [0.00174354]\n",
            "Loss: [0.00022313]\n",
            "Loss: [0.00121586]\n",
            "Loss: [0.00012698]\n",
            "Loss: [0.00019459]\n",
            "Loss: [0.00040873]\n",
            "Loss: [0.0001834]\n",
            "Loss: [0.00020186]\n",
            "Loss: [0.00025355]\n",
            "Loss: [0.00076955]\n",
            "Loss: [0.00026472]\n",
            "Loss: [0.0001838]\n",
            "Loss: [8.1130216e-05]\n",
            "Loss: [6.186286e-05]\n",
            "Loss: [0.0009585]\n",
            "Loss: [0.00021787]\n",
            "Loss: [0.00016296]\n",
            "Loss: [0.0002767]\n",
            "Loss: [0.00013373]\n",
            "Loss: [0.00010262]\n",
            "Loss: [0.00010971]\n",
            "Loss: [0.00032069]\n",
            "Completed epoch 3\n"
          ]
        }
      ],
      "source": [
        "epochs=3\n",
        "tf_dataset=to_tf(token_dataset).batch(1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for batch in tf_dataset:\n",
        "    inputs, labels = batch\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Forward pass\n",
        "      outputs = model_t5(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=labels)\n",
        "      loss = outputs.loss\n",
        "\n",
        "      # Backward pass\n",
        "      gradients = tape.gradient(loss, model_t5.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(gradients, model_t5.trainable_variables))\n",
        "\n",
        "      print(f\"Loss: {loss.numpy()}\")\n",
        "\n",
        "  print(f\"Completed epoch {epoch + 1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avXdt5X5WeMN"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from evaluate import load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Azd_yv30_0GM"
      },
      "outputs": [],
      "source": [
        "bleu_metric = load(\"bleu\")\n",
        "rouge_metric = load(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Hs9chnTgORCo"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "batch_size = 1\n",
        "tf_dataset = to_tf(token_dataset).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_IK6GgXWp4E"
      },
      "source": [
        "# Graph Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbVIKw89Wmbt"
      },
      "source": [
        "# Model Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqK4xXMS_0C4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
